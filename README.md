ğŸ”’ LLM Safety Tests
This repository provides a comprehensive test suite to evaluate Large Language Models (LLMs) across 7 critical dimensions of safety. Each test is grounded in leading academic benchmarks and datasets, allowing systematic, reproducible, and scalable LLM evaluations.

Included Safety Tests:

1)Content Harms
Evaluates generation of explicitly harmful content including toxicity, violence, and inappropriate material.
Dataset used: ToxiGen

2)Discrimination and Bias
Measures the model's bias across demographics, such as race, gender, religion, etc.
Dataset used: BBQ (Bias Benchmark for QA)

3)Misinformation and Disinformation
Tests for hallucination and the generation of false or misleading facts.
Dataset used: BEHONEST

4)Malicious Use Enablement
Assesses how easily an LLM assists in harmful, illegal, or dangerous activities.
Dataset used: HarmBench

5)Privacy Violations
Detects leakage of personally identifiable information (PII) or other sensitive content.
Dataset used: DecodingTrust

6)Interaction and Psychological Harms
Captures subtle harms in human-LLM interaction such as manipulation, over-reliance, and deception.
Dataset used: BEHONEST (Sycophancy)

7)Robustness and Reliability
Evaluates the modelâ€™s stability under adversarial prompts and resistance to manipulation.
Dataset used: AdvBench

âš™ï¸ Configuration & Customization

ğŸ”‘ API Keys:
Add your model providers' API keys (e.g., OpenAI, Anthropic, Google) in the .env file. 

ğŸ§  Add Custom LLMs:
Add or configure different LLMs in the config.yaml file. You can plug in models from OpenAI, Anthropic, HuggingFace, Google, etc.

ğŸ“‚ Datasets Loader:
All datasets used in these tests are modularized and managed via the datasets.py file. You can easily add new datasets or swap existing ones.

ğŸš€ Getting Started
Clone the repo:

git clone https://github.com/Arnav-SiriusAI/llm-safety-tests.git

cd llm-safety-tests

Install dependencies:

pip install -r requirements.txt


Set your .env with required API keys.

Run any test file to begin evaluation.

ğŸ“Œ Goals
This project aims to:

Promote transparency and safety in LLM deployment.

Provide reproducible, benchmark-based evaluations.

Support research on trustworthiness, robustness, and ethical AI.
